{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic\n",
    "\n",
    "Deep Neural Nets - Version 3\n",
    "\n",
    "I do the full set here\n",
    "1. He_Initialization\n",
    "2. Batch Normalization\n",
    "3. Nesterov Optimization and \n",
    "4. Drop-Out regularization\n",
    "\n",
    "Does not overfit.\n",
    "\n",
    "Bottom line - The amount of time this model takes to run vs. the time it takes for the regular models like Logistic-Regression to run is huge. So, it is a very good case to see why DNN is not a pill that solves all problems. Need to be careful about the models we use and also the way we use our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings \n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct the Training data\n",
    "#### Step1 - Change the categorical variables to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:6576: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "# Lets use Pclass, Sex, Age, SibSp, Parch, Fare, Embarked\n",
    "\n",
    "train_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].isnull().any()\n",
    "#Lets predict the age from the Fare, Sex, Pclass, parch and SubSp and use it\n",
    "\n",
    "selected_df = train_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'PassengerId', 'Survived']]\n",
    "\n",
    "#Sex\n",
    "selected_df['Sex'].replace('male',0, inplace=True)\n",
    "selected_df['Sex'].replace('female',1, inplace=True)\n",
    "\n",
    "#Embarked\n",
    "selected_df['Embarked'].replace('S', 0, inplace=True)\n",
    "selected_df['Embarked'].replace('C', 1, inplace=True)\n",
    "selected_df['Embarked'].replace('Q', 2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute for Predictors with missing values \n",
    "#### Step 2 - Use regression to predict the age missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3267: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Select sepcific rows\n",
    "age_select = selected_df.loc[selected_df['Age'].notnull(),['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "\n",
    "# Regression\n",
    "lmod_age = lm.LinearRegression()\n",
    "lmod_age.fit(age_select[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare']], age_select['Age'])\n",
    "\n",
    "selected_df['predicted_age'] = lmod_age.predict(selected_df[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare']])\n",
    "\n",
    "# Fill back the age correctly\n",
    "#selected_df[['Age', 'predicted_age']].Age\n",
    "#selected_df[['Age','predicted_age']].apply(lambda x: x)\n",
    "\n",
    "age = selected_df['Age']\n",
    "pred_age = selected_df['predicted_age']\n",
    "\n",
    "#selected_df['Age'] = np.where(math.isnan(age) is True, pred_age, age)\n",
    "\n",
    "for ind, row in selected_df.iterrows():\n",
    "    if math.isnan(row['Age']) is True:\n",
    "        if(row['predicted_age'] > 0 ):\n",
    "            selected_df['Age'][ind] = row['predicted_age']\n",
    "        else:\n",
    "            selected_df['Age'][ind] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct test data\n",
    "#### 1. Handle categorical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass    False\n",
       "Sex       False\n",
       "SibSp     False\n",
       "Parch     False\n",
       "Fare       True\n",
       "Age        True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Sex'].replace('male',0, inplace=True)\n",
    "test_df['Sex'].replace('female',1, inplace=True)\n",
    "\n",
    "test_df[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Age']].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# There is only one value missing in Fare and we adjust that as per ticket prices as per observation\n",
    "for ind, row in test_df.iterrows():\n",
    "    if math.isnan(row['Fare']) is True:\n",
    "        test_df['Fare'][ind] = 7.8958\n",
    "\n",
    "# Setting the age\n",
    "test_df['predicted_age'] = lmod_age.predict(test_df[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare']])\n",
    "\n",
    "# Correct Age\n",
    "for ind, row in test_df.iterrows():\n",
    "    if math.isnan(row['Age']) is True:\n",
    "        if(row['predicted_age'] > 0 ):\n",
    "            test_df['Age'][ind] = row['predicted_age']\n",
    "        else:\n",
    "            test_df['Age'][ind] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = selected_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "y_train = selected_df['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "[Name based Feature Engineering](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818)\n",
    "\n",
    "Refer to that article to understand how someone created a new feature based on family name groupings. We use that information to build a new XGBoost model and see if it bumps up our score. It is not worth spending more time than this. So, I will just attempt this one feature engineering attempt here.\n",
    "\n",
    "What these guys are doing is this\n",
    "* If a surname-woman-child combination is found to be alive in training set then they are borrowing that same surname-woman-child combination information to the test set\n",
    "* By default they are assuming that all women have survived. Which means they have only corrected for those cases where they definitely know the outcome from training set\n",
    "\n",
    "This looks a bit like gaming rather than modeling. But let us see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "x_train['Name'] = train_df['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('.*, (.*?)\\.')\n",
    "x_train['Title'] = x_train['Name'].apply(lambda x: p.match(x).group(1))\n",
    "\n",
    "# Change the title now to either man / woman\n",
    "x_train['Title'] = x_train['Title'].apply(lambda x: -1 if x in [\"Capt\",\"Don\",\"Major\",\"Col\",\"Rev\",\"Dr\",\"Sir\",\"Mr\", \"Jonkheer\"] \n",
    "                                          else 1 if x in [\"Dona\",\"the Countess\",\"Mme\",\"Mlle\",\"Ms\",\"Miss\",\"Lady\",\"Mrs\"]\n",
    "                                                 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:8672: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3267: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Engineer \"woman-child-groups\"\n",
    "p = re.compile('(.*?),.*')\n",
    "x_train['Surname'] = x_train['Name'].apply(lambda x: p.match(x).group(1))\n",
    "\n",
    "# Bucket people into three groups. All males go into no-group (-1)\n",
    "x_train.Surname[x_train.Title==-1] = -1\n",
    "\n",
    "# Check the bucket sizes for others\n",
    "surnamefreq = x_train.groupby(['Surname']).size()\n",
    "x_train['SurnameFreq'] = x_train['Surname'].apply(lambda x: surnamefreq[x])\n",
    "\n",
    "# If the family group size is just one then call that as no-group (-1)\n",
    "x_train['Surname'][x_train['SurnameFreq'] <= 1] = -1\n",
    "\n",
    "# Update frequencies one final time\n",
    "surnamefreq = x_train.groupby(['Surname']).size()\n",
    "x_train['SurnameFreq'] = x_train['Surname'].apply(lambda x: surnamefreq[x])\n",
    "\n",
    "# Now all the remaining women and children group has to be marked as a single group because mother and child fates\n",
    "# identified to be intertwined (in that shared link)\n",
    "\n",
    "# Now change Surnames and Titles into numeric data to have them be handled by XGBoost\n",
    "#x_train['Surname'][x_train['Surname'] != -1] = 1\n",
    "#x_train['SurnameClass'] = x_train['Surname'].apply(lambda x: int(x))\n",
    "\n",
    "# Now write the survival rate for the women-child-family name combinations\n",
    "x_train['Survived'] = y_train\n",
    "name_survival = x_train.groupby(['Surname','Survived']).size()\n",
    "\n",
    "for ind, row in x_train.iterrows():\n",
    "    surived_c = 0\n",
    "    not_survived_c = 0\n",
    "    \n",
    "    if((row['Surname'],1) in name_survival.index):\n",
    "        survived_c = name_survival[row['Surname'],1]\n",
    "    else:\n",
    "        survived_c = 0\n",
    "    \n",
    "    if((row['Surname'],0) in name_survival.index):\n",
    "        not_survived_c = name_survival[row['Surname'],0]\n",
    "    else:\n",
    "        not_survived_c = 0\n",
    "        \n",
    "    if(survived_c + not_survived_c > 0):\n",
    "        x_train.loc[ind,'SurnameSurvival'] = survived_c/(survived_c + not_survived_c)\n",
    "    else:\n",
    "        x_train.loc[ind,'SurnameSurvival'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that there is a slight bump in the cross validated score to 86.08%. Let us see if it really works.\n",
    "##### Engineering the features for Test set now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('.*, (.*?)\\.')\n",
    "test_df['Title'] = test_df['Name'].apply(lambda x: p.match(x).group(1))\n",
    "\n",
    "# Change the title now to either man / woman\n",
    "test_df['Title'] = test_df['Title'].apply(lambda x: -1 if x in [\"Capt\",\"Don\",\"Major\",\"Col\",\"Rev\",\"Dr\",\"Sir\",\"Mr\", \"Jonkheer\"] \n",
    "                                          else 1 if x in [\"Dona\",\"the Countess\",\"Mme\",\"Mlle\",\"Ms\",\"Miss\",\"Lady\",\"Mrs\"]\n",
    "                                                 else 0)\n",
    "\n",
    "# Engineer \"woman-child-groups\"\n",
    "p = re.compile('(.*?),.*')\n",
    "test_df['Surname'] = test_df['Name'].apply(lambda x: p.match(x).group(1))\n",
    "\n",
    "# Bucket people into three groups. All males go into no-group (-1)\n",
    "test_df.Surname[test_df.Title==-1] = -1\n",
    "\n",
    "# Check the bucket sizes for others\n",
    "surnamefreq = test_df.groupby(['Surname']).size()\n",
    "test_df['SurnameFreq'] = test_df['Surname'].apply(lambda x: surnamefreq[x])\n",
    "\n",
    "# If the family group size is just one then call that as no-group (-1)\n",
    "test_df['Surname'][test_df['SurnameFreq'] <= 1] = -1\n",
    "\n",
    "# Now all the remaining women and children group has to be marked as a single group because mother and child fates\n",
    "# identified to be intertwined (in that shared link)\n",
    "\n",
    "# Here borrow from training set\n",
    "test_df['SurnameSurvival'] = 0\n",
    "\n",
    "#test_df['Surname'].unique()\n",
    "\n",
    "for ind, row in test_df.iterrows():\n",
    "    if(row['Surname'] in x_train['Surname'].unique()):\n",
    "        test_df.loc[ind,'SurnameSurvival'] = x_train['SurnameSurvival'][x_train['Surname'] == row['Surname']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "data_train = scaler.fit_transform(x_train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'SurnameSurvival']])\n",
    "data_test = scaler.transform(test_df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'SurnameSurvival']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a TensorFlow model\n",
    "\n",
    "We have 7 predictors and these we will blow out into 16 dimensions and then we will reduce it to half every time.\n",
    "\n",
    "11 X 7 > 7 X 16 > 16 X 8 > 8 X 4 > 4 X 1\n",
    "\n",
    "#### This has\n",
    "1. 1 Input layer\n",
    "2. 3 Hidden layers\n",
    "3. 1 Output layer\n",
    "\n",
    "#### And output is 0 / 1\n",
    "\n",
    "Design the initializers, variables and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurons\n",
    "n_inputs = data_train.shape[1]\n",
    "n_hidden1 = 512\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 64\n",
    "n_outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, n_inputs])\n",
    "y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "\n",
    "# He Initilization\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "# Using tensor flow neuron_layer function\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    # Using Batch normalization\n",
    "    \n",
    "    # This operation outputs False nothing is fed to it. So, it will return true only during the training phase\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    \n",
    "    dropout_rate = 0.5 # 1 - keep_prob\n",
    "    \n",
    "    X_drop = tf.layers.dropout(X, dropout_rate, training=training) # Regualarize\n",
    "    \n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    bn1_drop = tf.layers.dropout(bn1_act, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_drop, n_hidden2, name=\"hidden2\", kernel_initializer=he_init)\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    bn2_drop = tf.layers.dropout(bn2_act, dropout_rate, training=training)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(bn2_drop, n_hidden3, name=\"hidden3\", kernel_initializer=he_init)\n",
    "    bn3 = tf.layers.batch_normalization(hidden3, training=training, momentum=0.9)\n",
    "    bn3_act = tf.nn.elu(bn3)\n",
    "    bn3_drop = tf.layers.dropout(bn3_act, dropout_rate, training=training)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn3_drop, n_outputs, name=\"outputs\", kernel_initializer=he_init)\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now start optimizing the cost function for the model\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report the overall accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get elements which are not between the specified indices\n",
    "# In our current implementation of the dnn, end_index is not included in the training set.\n",
    "# Hence I include it in the not-between set.\n",
    "def get_ele_notbetween(x_data, start_index, end_index):\n",
    "    data_last_index = x_data.shape[0]-1\n",
    "    \n",
    "    if(end_index >= data_last_index):\n",
    "        return x_data[0:start_index,]\n",
    "    elif(start_index<=0):\n",
    "        return x_data[end_index:data_last_index+1,]\n",
    "    else:\n",
    "        return np.concatenate((x_data[0:start_index,], x_data[end_index:data_last_index+1,]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Validation set accuracy: 0.49438202\n",
      "1001 Validation set accuracy: 0.8426966\n",
      "2001 Validation set accuracy: 0.8651685\n",
      "3001 Validation set accuracy: 0.83146065\n",
      "4001 Validation set accuracy: 0.83146065\n",
      "5001 Validation set accuracy: 0.8651685\n",
      "6001 Validation set accuracy: 0.83146065\n",
      "7001 Validation set accuracy: 0.85393256\n",
      "8001 Validation set accuracy: 0.8202247\n",
      "9001 Validation set accuracy: 0.85393256\n",
      "10001 Validation set accuracy: 0.8426966\n"
     ]
    }
   ],
   "source": [
    "#Execution Phase - Checking the model\n",
    "n_epochs = 10010\n",
    "\n",
    "# Total of 891 training samples exist. This is a small number. But let us keep 50 samples aside for testing\n",
    "total_samples = 891\n",
    "# Set aside 10% for validation\n",
    "val_set_size = 89\n",
    "batch_size = 890 - val_set_size\n",
    "number = np.random.randint(batch_size+1,high=total_samples-1)\n",
    "\n",
    "# This is required to update operations related to batch_normalization at each step during the training in order to\n",
    "# update the moving averages\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        number = np.random.randint(batch_size+1,high=total_samples-1)\n",
    "        sess.run([training_op, extra_update_ops], feed_dict={training: True, X: data_train[number-batch_size-1:number],y: y_train[number-batch_size-1:number]})\n",
    "        acc_train = accuracy.eval(feed_dict={ X: data_train[number-batch_size-1:number],y: y_train[number-batch_size-1:number]})\n",
    "        if(epoch%1000 == 1):\n",
    "            acc_test = accuracy.eval(feed_dict={ X: get_ele_notbetween(data_train,number-batch_size-1,number),y: get_ele_notbetween(y_train, number-batch_size-1, number)})\n",
    "            print(epoch, \"Validation set accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparison with other models\n",
    "\n",
    "Setting aside 10% of the data for validation we see a net accuracy of 85.38% for this model (after 5000 epochs). Though it is not exactly cross validation that I performed here, on average I expect it to perform the same as cross validation.\n",
    "\n",
    "Let me take a moment to compare this performance with other models. \n",
    "\n",
    "* Linear Regression : 81.26%\n",
    "* Random Forest (with Fare): 82.94% {'max_depth': 10, 'n_estimators': 14}\n",
    "* Random Forest (wo Fare): 83.05% {'max_depth': 5, 'n_estimators': 14}\n",
    "* SVC (wo Fare): 81.7% {'C': 1.2, 'kernel': 'rbf'}\n",
    "* XGBoost (with Fare): 83.6% {'alpha': 10, 'colsample_bytree': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 20, 'objective': 'reg:linear'}\n",
    "* XGBoost (wo Fare): 83.16% {'alpha': 10, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 25, 'objective': 'reg:linear'}\n",
    "\n",
    "Compared to all these models, DNN is giving us a better net result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How do I tune this model so that I do not overfit\n",
    "\n",
    "Let us take this approach that is suggested by Hands-on Machine Learning.\n",
    "1. Split data into training set and validation set\n",
    "2. Keep checking the rmse / accuracy on the validation as you keep training with the training set\n",
    "3. at one point the rmse / accuracy of the validation set will start going down whereas the error rate with training set will keep going down.\n",
    "4. That point of inflection is where you should stop training\n",
    "\n",
    "But since I am already using dropout for regularization, I am skipping the above as I am terribly starved for data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Training accuracy: 0.5768799\n",
      "1001 Training accuracy: 0.82828283\n",
      "2001 Training accuracy: 0.8271605\n",
      "3001 Training accuracy: 0.8260382\n",
      "4001 Training accuracy: 0.8305275\n",
      "5001 Training accuracy: 0.83501685\n",
      "6001 Training accuracy: 0.83501685\n",
      "7001 Training accuracy: 0.8338945\n",
      "8001 Training accuracy: 0.8271605\n",
      "9001 Training accuracy: 0.8271605\n",
      "10001 Training accuracy: 0.8237935\n"
     ]
    }
   ],
   "source": [
    "#Final model \n",
    "n_epochs = 10010\n",
    "\n",
    "# This is required to update operations related to batch_normalization at each step during the training in order to\n",
    "# update the moving averages\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run([training_op, extra_update_ops], feed_dict={training: True, X: data_train,y: y_train})\n",
    "        acc_train = accuracy.eval(feed_dict={ X: data_train,y: y_train})\n",
    "        if(epoch%1000 == 1):\n",
    "            print(epoch, \"Training accuracy:\", acc_train)\n",
    "    \n",
    "    pred = logits.eval(feed_dict={X: data_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test and proceed\n",
    "\n",
    "This one gives a 77.033% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to compute the softmax scores here. Probably if I compute them, I can use them to check the PR-Curves etc.\n",
    "# But I will directly consume the data as it is with whichever logit is higher that is the class-number (0/1)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pred_df = pd.DataFrame(pred)\n",
    "pred_df.columns = ['zero','one']\n",
    "\n",
    "final = pred_df.apply(lambda x : 1 if x['zero'] < x['one'] else 0, axis = 1).astype(int)\n",
    "\n",
    "test_df['Survived'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult = test_df[['PassengerId', 'Survived']]\n",
    "finalResult.to_csv(\"result_final_dnn3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Just making the final manual correction as we did previously to see if we can better our score by this adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Females\n",
    "for ind, row in test_df.iterrows():\n",
    "    if(row['Title'] == 1 and row['Survived'] == 0 and row['SurnameSurvival'] != 0):\n",
    "        test_df.loc[ind,'Survived'] = 1\n",
    "    if(row['Title'] == -1 and row['Survived'] == 1 and row['Sex'] == 0):\n",
    "        test_df.loc[ind,'Survived'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResult = test_df[['PassengerId', 'Survived']]\n",
    "finalResult.to_csv(\"result_final_dnn3_mc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfortunately this gives no bump in accuracy. But this concludes my take on applying DNN based classification to this trivial problem. But helps us see a bunch of techniques to tune and build DNNs and take care of issues like overfitting etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
