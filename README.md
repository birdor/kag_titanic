# kag_titanic
We discuss and evaluate multiple concepts and models here
1. Logistic regression
2. Random forest Classification
3. SVM classification
4. XGBoost
5. Imputation for missing data - Using regression to fill missing data
6. Feature Engineering

And with some inputs from another submission (referred to in the notebook) we achieve 77.99% accuracy 

On the leadership board there are submissions with higher accuracy, but I looked at one such solution and for higher accuracy people are handcrafting models (of their own) instead of using standard models. It may be good analysis. But if you want to learn different models and evaluation techniques then you can refer to my solution here. Also solutions with 100% accuracy on Kaggle are submissions based on Google lookup for list creation.
