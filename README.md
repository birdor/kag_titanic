# kag_titanic
We discuss and evaluate multiple concepts and models here
1. Logistic regression
2. Random forest Classification
3. SVM classification
4. XGBoost
5. Imputation for missing data - Using regression to fill missing data
6. Feature Engineering
7. Deep Neural Networks (uses Softmax layer for output)
	- Basic model (overfits and how it impacts accuracy)
	- Regularizing with DropOut method
	- Improving performance using Nesterov Descent and He-Initilization
	- Applying batch normalization to avoid vanishing gradients problem

And with some inputs from another submission (referred to in the notebook) we achieve 77.99% accuracy 

On the leadership board there are submissions with higher accuracy, but I looked at few solutions and for higher accuracy, contestants are identifying criteria that may not be generalizable. It may be good analysis but that is not my intention. The intention of these notebooks is to discuss various issues relating to model building and evlation. Hence, if you wish to learn different models and evaluation techniques then you can refer to my solution here.
